# ğŸ™ï¸ Real-Time Voice AI Assistant (Local LLM Powered)

Build your own **JARVIS-like voice assistant** using Python that listens to you, thinks using a **local Llama 3 model**, and speaks back in real time â€” **no cloud inference required**.

This project demonstrates how modern AI systems can replicate human sensory loops using simple, modular components.

---

## âœ¨ Features

* ğŸ§ **Speech-to-Text (Ears)** â€” Converts your voice into text
* ğŸ§  **Local AI Reasoning (Brain)** â€” Uses **Llama 3 via Ollama** for intelligent responses
* ğŸ”Š **Text-to-Speech (Mouth)** â€” Speaks responses naturally
* ğŸ”’ **Privacy-First** â€” All AI inference runs locally
* ğŸ” **Always-On Loop** â€” Continuous Listen â†’ Think â†’ Speak cycle
* âŒ **Graceful Exit** â€” Say *exit*, *stop*, or *quit* to shut down

---

## ğŸ§  How It Works

The assistant mimics three biological functions:

```
ğŸ™ï¸ Microphone
   â†“
Speech-to-Text (SpeechRecognition)
   â†“
LLM Reasoning (Llama 3 via Ollama)
   â†“
Text-to-Speech (pyttsx3)
   â†“
ğŸ”Š Speaker
```

Each interaction runs through this loop in real time.

---

## ğŸ› ï¸ Tech Stack

* **Python 3.9+**
* **SpeechRecognition** â€” Voice input
* **PyAudio** â€” Microphone access
* **Ollama** â€” Local LLM runtime
* **Llama 3** â€” Large Language Model
* **pyttsx3** â€” Offline Text-to-Speech

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/voice-ai-assistant.git
cd voice-ai-assistant
```

### 2ï¸âƒ£ Create & Activate Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install speechrecognition ollama pyttsx3 pyaudio
```

> âš ï¸ **PyAudio Installation Notes**

* macOS:

```bash
brew install portaudio
pip install pyaudio
```

* Windows: Install PyAudio wheel matching your Python version

---

## ğŸ§  Ollama Setup (Required)

1. Install Ollama:
   ğŸ‘‰ [https://ollama.com](https://ollama.com)

2. Pull the Llama 3 model:

```bash
ollama pull llama3
```

3. Make sure Ollama is running in the background.

---

## â–¶ï¸ Usage

Run the assistant:

```bash
python main.py
```

Youâ€™ll hear:

> *â€œHello, I am ready. You can start speaking.â€*

Speak naturally â€” the assistant will listen, think, and respond.

### ğŸ›‘ Exit Command

Say:

* `exit`
* `stop`
* `quit`

---

## ğŸš€ Future Improvements

* ğŸ” Streaming responses (real-time speech output)
* ğŸ—“ï¸ Calendar / Email / Task integrations
* ğŸ§  Memory using vector databases
* ğŸ¤– Wake-word detection
* ğŸŒ Replace Google STT with fully offline alternatives
* ğŸ–¥ï¸ GUI or desktop tray app

---

## ğŸ” Privacy Note

* Voice recognition uses **Google Web Speech API** (requires internet)
* **LLM inference is 100% local**
* No prompts are sent to cloud AI providers

---

## ğŸ¤ Contributing

Contributions, ideas, and improvements are welcome!

1. Fork the repo
2. Create a feature branch
3. Commit your changes
4. Open a Pull Request

---

## ğŸ“œ License

MIT License â€” free to use, modify, and distribute.

---

## â­ Final Thoughts

What once required massive infrastructure is now possible on a laptop.

This project is not just a chatbot â€” itâ€™s a **foundation for autonomous AI agents**.

**JARVIS isnâ€™t coming. Weâ€™re building it.** ğŸš€
